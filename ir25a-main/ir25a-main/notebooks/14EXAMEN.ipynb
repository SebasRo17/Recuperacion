{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0756526a",
   "metadata": {},
   "source": [
    "# Descarga del coprus y PREPROCESAMIENTO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55342715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb31552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 documentos cargados...\n",
      "20000 documentos cargados...\n",
      "30000 documentos cargados...\n",
      "40000 documentos cargados...\n",
      "50000 documentos cargados...\n",
      "60000 documentos cargados...\n",
      "70000 documentos cargados...\n",
      "80000 documentos cargados...\n",
      "90000 documentos cargados...\n",
      "100000 documentos cargados...\n",
      "110000 documentos cargados...\n",
      "120000 documentos cargados...\n",
      "130000 documentos cargados...\n",
      "140000 documentos cargados...\n",
      "150000 documentos cargados...\n",
      "160000 documentos cargados...\n",
      "170000 documentos cargados...\n",
      "180000 documentos cargados...\n",
      "190000 documentos cargados...\n",
      "200000 documentos cargados...\n",
      "210000 documentos cargados...\n",
      "220000 documentos cargados...\n",
      "230000 documentos cargados...\n",
      "240000 documentos cargados...\n",
      "250000 documentos cargados...\n",
      "260000 documentos cargados...\n",
      "270000 documentos cargados...\n",
      "280000 documentos cargados...\n",
      "290000 documentos cargados...\n",
      "300000 documentos cargados...\n",
      "310000 documentos cargados...\n",
      "320000 documentos cargados...\n",
      "330000 documentos cargados...\n",
      "340000 documentos cargados...\n",
      "350000 documentos cargados...\n",
      "360000 documentos cargados...\n",
      "370000 documentos cargados...\n",
      "380000 documentos cargados...\n",
      "390000 documentos cargados...\n",
      "400000 documentos cargados...\n",
      "410000 documentos cargados...\n",
      "420000 documentos cargados...\n",
      "430000 documentos cargados...\n",
      "440000 documentos cargados...\n",
      "450000 documentos cargados...\n",
      "460000 documentos cargados...\n",
      "470000 documentos cargados...\n",
      "480000 documentos cargados...\n",
      "490000 documentos cargados...\n",
      "500000 documentos cargados...\n",
      "510000 documentos cargados...\n",
      "520000 documentos cargados...\n",
      "530000 documentos cargados...\n",
      "540000 documentos cargados...\n",
      "550000 documentos cargados...\n",
      "560000 documentos cargados...\n",
      "570000 documentos cargados...\n",
      "580000 documentos cargados...\n",
      "590000 documentos cargados...\n",
      "600000 documentos cargados...\n",
      "610000 documentos cargados...\n",
      "620000 documentos cargados...\n",
      "630000 documentos cargados...\n",
      "640000 documentos cargados...\n",
      "650000 documentos cargados...\n",
      "660000 documentos cargados...\n",
      "670000 documentos cargados...\n",
      "680000 documentos cargados...\n",
      "690000 documentos cargados...\n",
      "700000 documentos cargados...\n",
      "710000 documentos cargados...\n",
      "720000 documentos cargados...\n",
      "730000 documentos cargados...\n",
      "740000 documentos cargados...\n",
      "750000 documentos cargados...\n",
      "760000 documentos cargados...\n",
      "770000 documentos cargados...\n",
      "780000 documentos cargados...\n",
      "790000 documentos cargados...\n",
      "800000 documentos cargados...\n",
      "810000 documentos cargados...\n",
      "820000 documentos cargados...\n",
      "830000 documentos cargados...\n",
      "840000 documentos cargados...\n",
      "850000 documentos cargados...\n",
      "860000 documentos cargados...\n",
      "870000 documentos cargados...\n",
      "880000 documentos cargados...\n",
      "890000 documentos cargados...\n",
      "900000 documentos cargados...\n",
      "910000 documentos cargados...\n",
      "920000 documentos cargados...\n",
      "930000 documentos cargados...\n",
      "940000 documentos cargados...\n",
      "950000 documentos cargados...\n",
      "960000 documentos cargados...\n",
      "970000 documentos cargados...\n",
      "980000 documentos cargados...\n",
      "990000 documentos cargados...\n",
      "1000000 documentos cargados...\n",
      "1010000 documentos cargados...\n",
      "1020000 documentos cargados...\n",
      "1030000 documentos cargados...\n",
      "1040000 documentos cargados...\n",
      "1050000 documentos cargados...\n",
      "1060000 documentos cargados...\n",
      "1070000 documentos cargados...\n",
      "1080000 documentos cargados...\n",
      "1090000 documentos cargados...\n",
      "1100000 documentos cargados...\n",
      "1110000 documentos cargados...\n",
      "1120000 documentos cargados...\n",
      "1130000 documentos cargados...\n",
      "1140000 documentos cargados...\n",
      "1150000 documentos cargados...\n",
      "1160000 documentos cargados...\n",
      "1170000 documentos cargados...\n",
      "1180000 documentos cargados...\n",
      "1190000 documentos cargados...\n",
      "1200000 documentos cargados...\n",
      "1210000 documentos cargados...\n",
      "1220000 documentos cargados...\n",
      "1230000 documentos cargados...\n",
      "1240000 documentos cargados...\n",
      "1250000 documentos cargados...\n",
      "1260000 documentos cargados...\n",
      "1270000 documentos cargados...\n",
      "1280000 documentos cargados...\n",
      "1290000 documentos cargados...\n",
      "1300000 documentos cargados...\n",
      "1310000 documentos cargados...\n",
      "1320000 documentos cargados...\n",
      "1330000 documentos cargados...\n",
      "1340000 documentos cargados...\n",
      "1350000 documentos cargados...\n",
      "1360000 documentos cargados...\n",
      "1370000 documentos cargados...\n",
      "1380000 documentos cargados...\n",
      "1390000 documentos cargados...\n",
      "1400000 documentos cargados...\n",
      "1410000 documentos cargados...\n",
      "1420000 documentos cargados...\n",
      "1430000 documentos cargados...\n",
      "1440000 documentos cargados...\n",
      "1450000 documentos cargados...\n",
      "1460000 documentos cargados...\n",
      "1470000 documentos cargados...\n",
      "1480000 documentos cargados...\n",
      "1490000 documentos cargados...\n",
      "1500000 documentos cargados...\n",
      "1510000 documentos cargados...\n",
      "1520000 documentos cargados...\n",
      "1530000 documentos cargados...\n",
      "1540000 documentos cargados...\n",
      "1550000 documentos cargados...\n",
      "1560000 documentos cargados...\n",
      "1570000 documentos cargados...\n",
      "1580000 documentos cargados...\n",
      "1590000 documentos cargados...\n",
      "1600000 documentos cargados...\n",
      "1610000 documentos cargados...\n",
      "1620000 documentos cargados...\n",
      "1630000 documentos cargados...\n",
      "1640000 documentos cargados...\n",
      "1650000 documentos cargados...\n",
      "1660000 documentos cargados...\n",
      "1670000 documentos cargados...\n",
      "1680000 documentos cargados...\n",
      "1690000 documentos cargados...\n",
      "1700000 documentos cargados...\n",
      "1710000 documentos cargados...\n",
      "1720000 documentos cargados...\n",
      "1730000 documentos cargados...\n",
      "1740000 documentos cargados...\n",
      "1750000 documentos cargados...\n",
      "1760000 documentos cargados...\n",
      "1770000 documentos cargados...\n",
      "1780000 documentos cargados...\n",
      "1790000 documentos cargados...\n",
      "1800000 documentos cargados...\n",
      "1810000 documentos cargados...\n",
      "1820000 documentos cargados...\n",
      "1830000 documentos cargados...\n",
      "1840000 documentos cargados...\n",
      "1850000 documentos cargados...\n",
      "1860000 documentos cargados...\n",
      "1870000 documentos cargados...\n",
      "1880000 documentos cargados...\n",
      "1890000 documentos cargados...\n",
      "1900000 documentos cargados...\n",
      "1910000 documentos cargados...\n",
      "1920000 documentos cargados...\n",
      "1930000 documentos cargados...\n",
      "1940000 documentos cargados...\n",
      "1950000 documentos cargados...\n",
      "1960000 documentos cargados...\n",
      "1970000 documentos cargados...\n",
      "1980000 documentos cargados...\n",
      "1990000 documentos cargados...\n",
      "2000000 documentos cargados...\n",
      "2010000 documentos cargados...\n",
      "2020000 documentos cargados...\n",
      "2030000 documentos cargados...\n",
      "2040000 documentos cargados...\n",
      "2050000 documentos cargados...\n",
      "2060000 documentos cargados...\n",
      "2070000 documentos cargados...\n",
      "2080000 documentos cargados...\n",
      "2090000 documentos cargados...\n",
      "2100000 documentos cargados...\n",
      "2110000 documentos cargados...\n",
      "2120000 documentos cargados...\n",
      "2130000 documentos cargados...\n",
      "2140000 documentos cargados...\n",
      "2150000 documentos cargados...\n",
      "2160000 documentos cargados...\n",
      "2170000 documentos cargados...\n",
      "2180000 documentos cargados...\n",
      "2190000 documentos cargados...\n",
      "2200000 documentos cargados...\n",
      "2210000 documentos cargados...\n",
      "2220000 documentos cargados...\n",
      "2230000 documentos cargados...\n",
      "2240000 documentos cargados...\n",
      "2250000 documentos cargados...\n",
      "2260000 documentos cargados...\n",
      "2270000 documentos cargados...\n",
      "2280000 documentos cargados...\n",
      "2290000 documentos cargados...\n",
      "2300000 documentos cargados...\n",
      "2310000 documentos cargados...\n",
      "2320000 documentos cargados...\n",
      "2330000 documentos cargados...\n",
      "2340000 documentos cargados...\n",
      "2350000 documentos cargados...\n",
      "2360000 documentos cargados...\n",
      "2370000 documentos cargados...\n",
      "2380000 documentos cargados...\n",
      "2390000 documentos cargados...\n",
      "2400000 documentos cargados...\n",
      "2410000 documentos cargados...\n",
      "2420000 documentos cargados...\n",
      "2430000 documentos cargados...\n",
      "2440000 documentos cargados...\n",
      "2450000 documentos cargados...\n",
      "2460000 documentos cargados...\n",
      "2470000 documentos cargados...\n",
      "2480000 documentos cargados...\n",
      "2490000 documentos cargados...\n",
      "2500000 documentos cargados...\n",
      "2510000 documentos cargados...\n",
      "2520000 documentos cargados...\n",
      "2530000 documentos cargados...\n",
      "2540000 documentos cargados...\n",
      "2550000 documentos cargados...\n",
      "2560000 documentos cargados...\n",
      "2570000 documentos cargados...\n",
      "2580000 documentos cargados...\n",
      "2590000 documentos cargados...\n",
      "2600000 documentos cargados...\n",
      "2610000 documentos cargados...\n",
      "2620000 documentos cargados...\n",
      "2630000 documentos cargados...\n",
      "2640000 documentos cargados...\n",
      "2650000 documentos cargados...\n",
      "2660000 documentos cargados...\n",
      "2670000 documentos cargados...\n",
      "2680000 documentos cargados...\n",
      "2690000 documentos cargados...\n",
      "2700000 documentos cargados...\n",
      "2710000 documentos cargados...\n",
      "2720000 documentos cargados...\n",
      "2730000 documentos cargados...\n",
      "2740000 documentos cargados...\n",
      "2750000 documentos cargados...\n",
      "2760000 documentos cargados...\n",
      "2770000 documentos cargados...\n",
      "2780000 documentos cargados...\n",
      "2790000 documentos cargados...\n",
      "Total documentos cargados: 2792339\n",
      "Muestra (1%): 27923 documentos\n",
      "Guardado: C:\\Users\\roble\\OneDrive\\Documentos\\GitHub\\Recuperacion\\ir25a-main\\ir25a-main\\data\\EXAMEN\\corpus_1_percent.jsonl\n",
      "Guardado: C:\\Users\\roble\\OneDrive\\Documentos\\GitHub\\Recuperacion\\ir25a-main\\ir25a-main\\data\\EXAMEN\\corpus_1_percent.json\n",
      "\n",
      "Campos del primer documento: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
      "\n",
      "Ejemplo de títulos:\n",
      "  1. Universal homotopy theories...\n",
      "  2. The dynamics of correlated novelties...\n",
      "  3. Pomeron loops in the perturbative QCD with Large N_c...\n",
      "\n",
      "Ejemplo de abstracts:\n",
      "  1.   Given a small category C, we show that there is a universal way of expanding\n",
      "C into a model category, essentially by formally adjoining homotopy col...\n",
      "  2.   One new thing often leads to another. Such correlated novelties are a\n",
      "familiar part of daily life. They are also thought to be fundamental to the\n",
      "ev...\n"
     ]
    }
   ],
   "source": [
    "corpus_path = r\"C:\\Users\\roble\\OneDrive\\Documentos\\GitHub\\Recuperacion\\ir25a-main\\ir25a-main\\data\\EXAMEN\\arxiv-metadata-oai-snapshot.json\"\n",
    "base_path = r\"C:\\Users\\roble\\OneDrive\\Documentos\\GitHub\\Recuperacion\\ir25a-main\\ir25a-main\\data\\EXAMEN\"\n",
    "output_jsonl = os.path.join(base_path, \"corpus_1_percent.jsonl\")\n",
    "output_json = os.path.join(base_path, \"corpus_1_percent.json\")\n",
    "\n",
    "if not os.path.exists(corpus_path):\n",
    "    raise FileNotFoundError(f\"No se encontró el archivo en la ruta: {corpus_path}\")\n",
    "\n",
    "# --- Leer corpus como JSON Lines ---\n",
    "documents = []\n",
    "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file, 1):\n",
    "        try:\n",
    "            doc = json.loads(line)\n",
    "            documents.append(doc)\n",
    "            if i % 10000 == 0:\n",
    "                print(f\"{i} documentos cargados...\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            if i <= 5:\n",
    "                print(f\"Error en línea {i}: {e} (parcial: {line[:80]}...)\")\n",
    "            continue\n",
    "\n",
    "total_docs = len(documents)\n",
    "print(f\"Total documentos cargados: {total_docs}\")\n",
    "\n",
    "# --- Muestreo aleatorio del 1% del corpus ---\n",
    "sample_size = max(1, int(total_docs * 0.01))\n",
    "random.seed(42)\n",
    "sample_docs = random.sample(documents, sample_size)\n",
    "print(f\"Muestra (1%): {sample_size} documentos\")\n",
    "\n",
    "# --- Guardar la muestra en JSON Lines y JSON ---\n",
    "with open(output_jsonl, 'w', encoding='utf-8') as file:\n",
    "    for doc in sample_docs:\n",
    "        json.dump(doc, file, ensure_ascii=False)\n",
    "        file.write('\\n')\n",
    "print(f\"Guardado: {output_jsonl}\")\n",
    "\n",
    "with open(output_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(sample_docs, file, indent=2, ensure_ascii=False)\n",
    "print(f\"Guardado: {output_json}\")\n",
    "\n",
    "# --- Mostrar algunos ejemplos para verificar ---\n",
    "if sample_docs:\n",
    "    print(\"\\nCampos del primer documento:\", list(sample_docs[0].keys()))\n",
    "    print(\"\\nEjemplo de títulos:\")\n",
    "    for i, doc in enumerate(sample_docs[:3]):\n",
    "        print(f\"  {i+1}. {doc.get('title', '')[:100]}...\")\n",
    "    print(\"\\nEjemplo de abstracts:\")\n",
    "    for i, doc in enumerate(sample_docs[:2]):\n",
    "        print(f\"  {i+1}. {doc.get('abstract', '')[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a43cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40465163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos cargados: 27923\n",
      "\n",
      "DataFrame original (sin limpiar):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math/0007070</td>\n",
       "      <td>Universal homotopy theories</td>\n",
       "      <td>Given a small category C, we show that there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1310.1953</td>\n",
       "      <td>The dynamics of correlated novelties</td>\n",
       "      <td>One new thing often leads to another. Such c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0901.3660</td>\n",
       "      <td>Pomeron loops in the perturbative QCD with Lar...</td>\n",
       "      <td>The lowest order pomeron loop is calculated ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0  math/0007070                        Universal homotopy theories   \n",
       "1     1310.1953               The dynamics of correlated novelties   \n",
       "2     0901.3660  Pomeron loops in the perturbative QCD with Lar...   \n",
       "\n",
       "                                            abstract  \n",
       "0    Given a small category C, we show that there...  \n",
       "1    One new thing often leads to another. Such c...  \n",
       "2    The lowest order pomeron loop is calculated ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame con texto limpio (solo las 3 primeras filas):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_title</th>\n",
       "      <th>original_abstract</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math/0007070</td>\n",
       "      <td>Universal homotopy theories</td>\n",
       "      <td>Given a small category C, we show that there...</td>\n",
       "      <td>universal homotopy theories given small catego...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1310.1953</td>\n",
       "      <td>The dynamics of correlated novelties</td>\n",
       "      <td>One new thing often leads to another. Such c...</td>\n",
       "      <td>dynamics correlated novelties one new thing of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0901.3660</td>\n",
       "      <td>Pomeron loops in the perturbative QCD with Lar...</td>\n",
       "      <td>The lowest order pomeron loop is calculated ...</td>\n",
       "      <td>pomeron loops perturbative qcd large lowest or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                     original_title  \\\n",
       "0  math/0007070                        Universal homotopy theories   \n",
       "1     1310.1953               The dynamics of correlated novelties   \n",
       "2     0901.3660  Pomeron loops in the perturbative QCD with Lar...   \n",
       "\n",
       "                                   original_abstract  \\\n",
       "0    Given a small category C, we show that there...   \n",
       "1    One new thing often leads to another. Such c...   \n",
       "2    The lowest order pomeron loop is calculated ...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  universal homotopy theories given small catego...  \n",
       "1  dynamics correlated novelties one new thing of...  \n",
       "2  pomeron loops perturbative qcd large lowest or...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de documentos procesados: 27923\n",
      "Promedio de tokens por documento: 95.43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Prepara stopwords solo una vez ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# --- Ruta del corpus de muestra ---\n",
    "sample_path = r\"C:\\Users\\roble\\OneDrive\\Documentos\\GitHub\\Recuperacion\\ir25a-main\\ir25a-main\\data\\EXAMEN\\corpus_1_percent.json\"\n",
    "\n",
    "if not os.path.exists(sample_path):\n",
    "    raise FileNotFoundError(f\"No se encontró el archivo de muestra en: {sample_path}\")\n",
    "\n",
    "# --- Cargar documentos de muestra ---\n",
    "with open(sample_path, 'r', encoding='utf-8') as file:\n",
    "    sample_docs = json.load(file)\n",
    "\n",
    "print(f\"Documentos cargados: {len(sample_docs)}\")\n",
    "\n",
    "# --- DataFrame ORIGINAL (sin limpiar) ---\n",
    "df_original = pd.DataFrame([{\n",
    "    'id': doc.get('id', ''),\n",
    "    'title': doc.get('title', ''),\n",
    "    'abstract': doc.get('abstract', '')\n",
    "} for doc in sample_docs])\n",
    "\n",
    "print(\"\\nDataFrame original (sin limpiar):\")\n",
    "display(df_original.head(3))\n",
    "\n",
    "# --- Preprocesamiento y creación de texto limpio ---\n",
    "def tokens_to_text(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "processed_corpus = []\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    title = doc.get('title', '')\n",
    "    abstract = doc.get('abstract', '')\n",
    "    title_tokens = preprocess_text(title)\n",
    "    abstract_tokens = preprocess_text(abstract)\n",
    "    combined_tokens = title_tokens + abstract_tokens\n",
    "    processed_corpus.append({\n",
    "        'id': doc.get('id', f'doc_{i}'),\n",
    "        'original_title': title,\n",
    "        'original_abstract': abstract,\n",
    "        'text_clean': tokens_to_text(combined_tokens)\n",
    "    })\n",
    "\n",
    "df_clean = pd.DataFrame(processed_corpus)\n",
    "\n",
    "print(\"\\nDataFrame con texto limpio (solo las 3 primeras filas):\")\n",
    "display(df_clean[['id', 'original_title', 'original_abstract', 'text_clean']].head(3))\n",
    "\n",
    "# --- Estadísticas útiles ---\n",
    "print(f\"\\nTotal de documentos procesados: {len(df_clean)}\")\n",
    "print(f\"Promedio de tokens por documento: {df_clean['text_clean'].str.split().apply(len).mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002a550",
   "metadata": {},
   "source": [
    "# Indexacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf079c71",
   "metadata": {},
   "source": [
    "## TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d6aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f5d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos a indexar: 27923\n",
      "TF-IDF listo: documentos=27923, términos únicos=5000\n",
      "Densidad de la matriz: 0.0119\n",
      "Primeros 10 términos del vocabulario: ['abelian' 'abilities' 'ability' 'ablation' 'able' 'absence' 'absent'\n",
      " 'absolute' 'absorbing' 'absorption']\n"
     ]
    }
   ],
   "source": [
    "# --- Ruta del corpus preprocesado ---\n",
    "preprocessed_path = r\"C:\\Users\\roble\\OneDrive\\Documentos\\GitHub\\Recuperacion\\ir25a-main\\ir25a-main\\data\\EXAMEN\\corpus_preprocessed.json\"\n",
    "\n",
    "if not os.path.exists(preprocessed_path):\n",
    "    raise FileNotFoundError(f\"No se encontró el archivo: {preprocessed_path}\")\n",
    "\n",
    "# --- Cargar corpus preprocesado ---\n",
    "with open(preprocessed_path, 'r', encoding='utf-8') as file:\n",
    "    processed_corpus = json.load(file)\n",
    "\n",
    "# --- Preparar textos para TF-IDF ---\n",
    "documents = [' '.join(doc['combined_tokens']) for doc in processed_corpus]\n",
    "doc_ids = [doc['id'] for doc in processed_corpus]\n",
    "\n",
    "print(f\"Documentos a indexar: {len(documents)}\")\n",
    "\n",
    "# --- Crear y entrenar vectorizador TF-IDF ---\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,        # Ya está preprocesado\n",
    "    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "    max_features=5000\n",
    ")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(f\"TF-IDF listo: documentos={tfidf_matrix.shape[0]}, términos únicos={tfidf_matrix.shape[1]}\")\n",
    "print(f\"Densidad de la matriz: {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")\n",
    "print(\"Primeros 10 términos del vocabulario:\", tfidf_vectorizer.get_feature_names_out()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844d8af",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7bad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando índice BM25 para 27923 documentos...\n",
      "Índice BM25 creado:\n",
      "- Documentos: 27923\n",
      "- Vocabulario: 27923 términos únicos\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Usar los mismos documentos tokenizados del TF-IDF\n",
    "tokenized_docs = []\n",
    "for doc in processed_corpus:\n",
    "    tokenized_docs.append(doc['combined_tokens'])\n",
    "\n",
    "print(f\"Creando índice BM25 para {len(tokenized_docs)} documentos...\")\n",
    "\n",
    "# Crear índice BM25\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "print(f\"Índice BM25 creado:\")\n",
    "print(f\"- Documentos: {len(tokenized_docs)}\")\n",
    "print(f\"- Vocabulario: {len(bm25.doc_freqs)} términos únicos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b5926",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e34ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a620f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando embeddings para 27923 documentos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f94cc5855f47cb91f5455c89b81cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados:\n",
      "- Dimensión: 384\n",
      "- Documentos: 27923\n",
      "\n",
      "Índice FAISS creado:\n",
      "- Total vectores indexados: 27923\n",
      "- Dimensión: 384\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Preparar textos para embeddings (usar documentos preprocesados)\n",
    "texts = []\n",
    "for doc in processed_corpus:\n",
    "    # Combinar título y abstract originales\n",
    "    text = doc['original_title'] + ' ' + doc['original_abstract']\n",
    "    texts.append(text)\n",
    "\n",
    "print(f\"Generando embeddings para {len(texts)} documentos...\")\n",
    "\n",
    "# Generar embeddings\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings generados:\")\n",
    "print(f\"- Dimensión: {embeddings.shape[1]}\")\n",
    "print(f\"- Documentos: {embeddings.shape[0]}\")\n",
    "\n",
    "# Crear índice FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product para similitud coseno\n",
    "\n",
    "# Normalizar embeddings para similitud coseno\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Añadir embeddings al índice\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(f\"\\nÍndice FAISS creado:\")\n",
    "print(f\"- Total vectores indexados: {index.ntotal}\")\n",
    "print(f\"- Dimensión: {dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f475e2c",
   "metadata": {},
   "source": [
    "# Recuperacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e642dff",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "108fc3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def search_tfidf(query, top_k=10):\n",
    "    query_vec = tfidf_vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [{'id': doc_ids[i], 'score': scores[i]} for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d84ada",
   "metadata": {},
   "source": [
    "## BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0308fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def search_bm25(query, top_k=10):\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [{'id': doc_ids[i], 'score': scores[i]} for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ca8c",
   "metadata": {},
   "source": [
    "## FAIISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ea9d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, top_k=10):\n",
    "    query_emb = model.encode([query]).astype('float32')  # <- usas 'model' no 'sentence_model'\n",
    "    faiss.normalize_L2(query_emb)  # <- porque tu índice está normalizado\n",
    "    distances, indices = index.search(query_emb, top_k)  # <- usas 'index' no 'faiss_index'\n",
    "    return [{'id': doc_ids[i], 'distance': distances[0][rank]} for rank, i in enumerate(indices[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cfd6f",
   "metadata": {},
   "source": [
    "### DataFrame comparativo de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d741f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>TF-IDF ∩ BM25</th>\n",
       "      <th>TF-IDF ∩ FAISS</th>\n",
       "      <th>BM25 ∩ FAISS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers in natural language processing</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deep reinforcement learning algorithms</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adversarial attacks in computer vision</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>graph neural networks applications</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explainable artificial intelligence methods</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Query  TF-IDF ∩ BM25  TF-IDF ∩ FAISS  \\\n",
       "0  transformers in natural language processing              4               1   \n",
       "1       deep reinforcement learning algorithms              4               2   \n",
       "2       adversarial attacks in computer vision              6               2   \n",
       "3           graph neural networks applications              8               3   \n",
       "4  explainable artificial intelligence methods              5               1   \n",
       "\n",
       "   BM25 ∩ FAISS  \n",
       "0             2  \n",
       "1             3  \n",
       "2             2  \n",
       "3             3  \n",
       "4             1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Leer queries ---\n",
    "queries_path = r\"C:\\Users\\roble\\OneDrive\\Documentos\\GitHub\\Recuperacion\\ir25a-main\\ir25a-main\\data\\queries\\queries.txt\"\n",
    "with open(queries_path, 'r', encoding='utf-8') as f:\n",
    "    queries = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# --- 2. Buscar Top 10 IDs para cada query con cada modelo ---\n",
    "tfidf_top10_by_query = []\n",
    "bm25_top10_by_query = []\n",
    "faiss_top10_by_query = []\n",
    "\n",
    "for query in queries:\n",
    "    tfidf_result = search_tfidf(query, top_k=10)\n",
    "    bm25_result = search_bm25(query, top_k=10)\n",
    "    faiss_result = search_faiss(query, top_k=10)\n",
    "\n",
    "    tfidf_top10_by_query.append([doc['id'] for doc in tfidf_result])\n",
    "    bm25_top10_by_query.append([doc['id'] for doc in bm25_result])\n",
    "    faiss_top10_by_query.append([doc['id'] for doc in faiss_result])\n",
    "\n",
    "# --- 3. Comparar documentos comunes entre modelos ---\n",
    "comparaciones = []\n",
    "for i, query in enumerate(queries):\n",
    "    tfidf_set = set(tfidf_top10_by_query[i])\n",
    "    bm25_set = set(bm25_top10_by_query[i])\n",
    "    faiss_set = set(faiss_top10_by_query[i])\n",
    "\n",
    "    comunes_tfidf_bm25 = len(tfidf_set & bm25_set)\n",
    "    comunes_tfidf_faiss = len(tfidf_set & faiss_set)\n",
    "    comunes_bm25_faiss = len(bm25_set & faiss_set)\n",
    "\n",
    "    comparaciones.append({\n",
    "        'Query': query,\n",
    "        'TF-IDF ∩ BM25': comunes_tfidf_bm25,\n",
    "        'TF-IDF ∩ FAISS': comunes_tfidf_faiss,\n",
    "        'BM25 ∩ FAISS': comunes_bm25_faiss\n",
    "    })\n",
    "\n",
    "# --- 4. Mostrar resultados ---\n",
    "df_comparacion = pd.DataFrame(comparaciones)\n",
    "df_comparacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e315da2",
   "metadata": {},
   "source": [
    "### Análisis de intersección entre métodos de recuperación\n",
    "\n",
    "En la comparación de los resultados obtenidos por los tres métodos de recuperación (TF-IDF, BM25 y FAISS) para las cinco consultas analizadas, se observan varios patrones interesantes:\n",
    "\n",
    "- **TF-IDF y BM25** muestran una mayor coincidencia entre sí. En consultas como `\"graph neural networks applications\"` se observa una coincidencia de **8 documentos sobre 10**, lo que refleja una alta similitud en los criterios de relevancia de ambos modelos basados en estadística de términos.\n",
    "  \n",
    "- La intersección entre **TF-IDF y FAISS** es considerablemente más baja, con un máximo de **3 coincidencias**, lo cual es esperable ya que FAISS se basa en embeddings semánticos y no en ocurrencias de palabras clave.\n",
    "\n",
    "- **BM25 y FAISS** también presentan coincidencias moderadas (hasta 3 documentos en común), pero en general muestran menos alineación que BM25 con TF-IDF.\n",
    "\n",
    "- En ningún caso se presenta una coincidencia perfecta entre los tres métodos al mismo tiempo, lo que indica que **cada modelo prioriza documentos distintos** según su forma de evaluar la similitud.\n",
    "\n",
    "Este análisis evidencia que TF-IDF y BM25, al ser métodos basados en términos, tienden a recuperar documentos similares, mientras que FAISS aporta diversidad al enfocarse en la semántica del texto. Esto justifica el uso de modelos complementarios en sistemas de recuperación híbridos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b32a9",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572a75f",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "API_KEY = \"\"\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2746c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt enviado al modelo ===\n",
      "\n",
      "Eres un experto en artículos científicos. Usa SOLO la información del siguiente contexto para responder la pregunta del usuario. Si no está la respuesta, di explícitamente que no sabes.\n",
      "\n",
      "Contexto recuperado:\n",
      "[Doc 1] ID: 1810.11772\n",
      "Título: Learning with Analytical Models\n",
      "Resumen:   To understand and predict the performance of scientific applications, several\n",
      "analytical and machine learning approaches have been proposed, each having its\n",
      "advantages and disadvantages. In this paper, we propose and validate a hybrid\n",
      "approach for performance modeling and prediction, which combines analytical and\n",
      "machine learning models. The proposed hybrid model aims to minimize prediction\n",
      "cost while providing reasonable prediction accuracy. Our validation results\n",
      "show that the hybrid model is able to learn and correct the analytical models\n",
      "to better match the actual performance. Furthermore, the proposed hybrid model\n",
      "improves the prediction accuracy in comparison to pure machine learning\n",
      "techniques while using small training datasets, thus making it suitable for\n",
      "hardware and workload changes.\n",
      "\n",
      "\n",
      "[Doc 2] ID: 2211.05085\n",
      "Título: Automated Learning: An Implementation of The A* Search Algorithm over\n",
      "  The Random Base Functions\n",
      "Resumen:   This letter explains an algorithm for finding a set of base functions. The\n",
      "method aims to capture the leading behavior of the dataset in terms of a few\n",
      "base functions. Implementation of the A-star search will help find these\n",
      "functions, while the gradient descent optimizes the parameters of the functions\n",
      "at each search step. We will show the resulting plots to compare the\n",
      "extrapolation with the unseen data.\n",
      "\n",
      "\n",
      "[Doc 3] ID: 1402.2300\n",
      "Título: Feature and Variable Selection in Classification\n",
      "Resumen:   The amount of information in the form of features and variables avail- able\n",
      "to machine learning algorithms is ever increasing. This can lead to classifiers\n",
      "that are prone to overfitting in high dimensions, high di- mensional models do\n",
      "not lend themselves to interpretable results, and the CPU and memory resources\n",
      "necessary to run on high-dimensional datasets severly limit the applications of\n",
      "the approaches. Variable and feature selection aim to remedy this by finding a\n",
      "subset of features that in some way captures the information provided best. In\n",
      "this paper we present the general methodology and highlight some specific\n",
      "approaches.\n",
      "\n",
      "\n",
      "\n",
      "Pregunta del usuario: machine learning\n",
      "\n",
      "Respuesta:\n",
      "\n",
      "=== Respuesta generada (RAG) ===\n",
      "\n",
      "El contexto proporciona información sobre enfoques de aprendizaje automático en relación con modelos analíticos y la selección de características y variables. Se menciona un modelo híbrido que combina enfoques analíticos y de aprendizaje automático para mejorar la precisión de predicción y se discute la importancia de la selección de características para evitar el sobreajuste en modelos de alta dimensión.\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepara la consulta y extrae los top-3 FAISS\n",
    "consulta_rag = \"machine learning\"\n",
    "top3_docs = search_faiss(consulta_rag, top_k=3)\n",
    "\n",
    "# 2. Encuentra los documentos completos para cada ID\n",
    "contexto = \"\"\n",
    "for i, doc_info in enumerate(top3_docs):\n",
    "    doc_id = doc_info['id']\n",
    "    # Buscar el documento completo en el corpus procesado\n",
    "    original_doc = None\n",
    "    for doc in processed_corpus:\n",
    "        if doc['id'] == doc_id:\n",
    "            original_doc = doc\n",
    "            break\n",
    "    \n",
    "    if original_doc:\n",
    "        contexto += f\"[Doc {i+1}] ID: {doc_id}\\nTítulo: {original_doc['original_title']}\\nResumen: {original_doc['original_abstract']}\\n\\n\"\n",
    "    else:\n",
    "        contexto += f\"[Doc {i+1}] ID: {doc_id}\\n(Documento completo no encontrado)\\n\\n\"\n",
    "\n",
    "prompt_rag = (\n",
    "    f\"Eres un experto en artículos científicos. Usa SOLO la información del siguiente contexto para responder \"\n",
    "    f\"la pregunta del usuario. Si no está la respuesta, di explícitamente que no sabes.\\n\\n\"\n",
    "    f\"Contexto recuperado:\\n{contexto}\\n\"\n",
    "    f\"Pregunta del usuario: {consulta_rag}\\n\\n\"\n",
    "    f\"Respuesta:\"\n",
    ")\n",
    "\n",
    "# 3. Llama a la API de OpenAI\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Responde de forma clara y concisa, usando SOLO el contexto proporcionado.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_rag}\n",
    "    ],\n",
    "    max_tokens=350,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "respuesta_rag = response.choices[0].message.content\n",
    "\n",
    "print(\"=== Prompt enviado al modelo ===\\n\")\n",
    "print(prompt_rag)\n",
    "print(\"\\n=== Respuesta generada (RAG) ===\\n\")\n",
    "print(respuesta_rag)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
