{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e92cd0550dcfd1",
   "metadata": {},
   "source": [
    "# Ejercicio 5: Modelo Probabilístico\n",
    "\n",
    "## Objetivo de la práctica\n",
    "- Aplicar paso a paso técnicas de preprocesamiento, evaluando el impacto de cada etapa en el número de tokens y en el vocabulario final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88982921c8872f8f",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T15:51:10.347651Z",
     "start_time": "2025-05-28T15:51:07.548869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroupsdocs = newsgroups.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6196ea9cb414396",
   "metadata": {},
   "source": [
    "## Parte 1: Tokenización\n",
    "\n",
    "### Actividad \n",
    "1. Tokeniza los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb0f2438c9c0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043ff43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID Documento  Número de tokens  \\\n",
      "0             0               137   \n",
      "1             1                53   \n",
      "2             2               241   \n",
      "3             3               144   \n",
      "4             4               125   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [I, am, sure, some, bashers, of, Pens, fans, a...  \n",
      "1  [My, brother, is, in, the, market, for, a, hig...  \n",
      "2  [Finally, you, said, what, you, dream, about.,...  \n",
      "3  [Think!, It's, the, SCSI, card, doing, the, DM...  \n",
      "4  [1), I, have, an, old, Jasmine, drive, which, ...  \n"
     ]
    }
   ],
   "source": [
    "# tokenizar\n",
    "tokenized_docs = [doc.split() for doc in newsgroupsdocs]\n",
    "\n",
    "# armar DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'ID Documento': range(len(tokenized_docs)),\n",
    "    'Número de tokens': [len(toks) for toks in tokenized_docs],\n",
    "    'Tokens': tokenized_docs\n",
    "})\n",
    "\n",
    "# mostrar primeras filas\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecfc1e6638e9d2",
   "metadata": {},
   "source": [
    "## Parte 2: Normalización\n",
    "\n",
    "### Actividad \n",
    "1. Convierte todos los tokens a minúsculas.\n",
    "2. Elimina puntuación y símbolos no alfabéticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc67a424c6550fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID Documento  Número de tokens  \\\n",
      "0             0               137   \n",
      "1             1                47   \n",
      "2             2               236   \n",
      "3             3               144   \n",
      "4             4               117   \n",
      "\n",
      "                                 Tokens Normalizados  \n",
      "0  [i, am, sure, some, bashers, of, pens, fans, a...  \n",
      "1  [my, brother, is, in, the, market, for, a, hig...  \n",
      "2  [finally, you, said, what, you, dream, about, ...  \n",
      "3  [think, its, the, scsi, card, doing, the, dma,...  \n",
      "4  [i, have, an, old, jasmine, drive, which, i, c...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# asumiendo que ya tienes tokenized_docs de la parte 1\n",
    "\n",
    "normalized_docs = []\n",
    "for tokens in tokenized_docs:\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # 1) pasar a minúsculas\n",
    "        lower = tok.lower()\n",
    "        # 2) eliminar todo lo que no sean letras\n",
    "        only_letters = re.sub(r'[^a-z]', '', lower)\n",
    "        if only_letters:\n",
    "            clean_tokens.append(only_letters)\n",
    "    normalized_docs.append(clean_tokens)\n",
    "\n",
    "# crear DataFrame de normalización\n",
    "df_norm = pd.DataFrame({\n",
    "    'ID Documento': range(len(normalized_docs)),\n",
    "    'Número de tokens': [len(toks) for toks in normalized_docs],\n",
    "    'Tokens Normalizados': normalized_docs\n",
    "})\n",
    "\n",
    "print(df_norm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973153ad553db841",
   "metadata": {},
   "source": [
    "## Parte 3: Eliminación de Stopwords\n",
    "\n",
    "### Actividad \n",
    "1. Elimina las palabras vacías usando una lista estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477c7bcd5c2d0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID Documento  Número de tokens  \\\n",
      "0             0                63   \n",
      "1             1                31   \n",
      "2             2               109   \n",
      "3             3                70   \n",
      "4             4                46   \n",
      "\n",
      "                                    Tokens Filtrados  \n",
      "0  [sure, bashers, pens, fans, pretty, confused, ...  \n",
      "1  [brother, market, highperformance, video, card...  \n",
      "2  [finally, said, dream, mediterranean, new, are...  \n",
      "3  [think, scsi, card, doing, dma, transfers, dis...  \n",
      "4  [old, jasmine, drive, use, new, understanding,...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# asumiendo que ya tienes normalized_docs de la parte 2\n",
    "stopwords = ENGLISH_STOP_WORDS\n",
    "\n",
    "filtered_docs = []\n",
    "for tokens in normalized_docs:\n",
    "    filtered = [tok for tok in tokens if tok not in stopwords]\n",
    "    filtered_docs.append(filtered)\n",
    "\n",
    "df_no_stop = pd.DataFrame({\n",
    "    'ID Documento': range(len(filtered_docs)),\n",
    "    'Número de tokens': [len(toks) for toks in filtered_docs],\n",
    "    'Tokens Filtrados': filtered_docs\n",
    "})\n",
    "\n",
    "print(df_no_stop.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f090bfc7868f8",
   "metadata": {},
   "source": [
    "## Parte 4: Stemming o Lematización\n",
    "\n",
    "### Actividad\n",
    "1. Aplica stemming.\n",
    "2. Aplica lematización.\n",
    "3. Compara ambas técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ff693047bd948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
