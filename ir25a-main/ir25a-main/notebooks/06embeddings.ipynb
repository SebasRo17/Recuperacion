{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f8b5c16e7eb563",
   "metadata": {},
   "source": [
    "# Ejercicio 6: Dense Retrieval e Introducción a FAISS\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "Generar embeddings con sentence-transformers (SBERT, E5), e indexar documentos con FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd69ed7fcbeef9d",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus\n",
    "### Actividad\n",
    "\n",
    "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
    "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00fbde6cfc88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de documentos en el corpus: 2000\n"
     ]
    }
   ],
   "source": [
    "# Importamos la función para obtener el dataset 20 Newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#Cargamos el corpus completo (todos los subsets) y eliminamos headers, footers y quotes\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "corpus = newsgroups.data\n",
    "#Limitamos el corpus a los primeros 2000 documentos para facilitar el procesamiento\n",
    "corpus = corpus[:2000]\n",
    "# Verificamos cuántos documentos tenemos\n",
    "print(f'Número de documentos en el corpus: {len(corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9184f4b3e66e20a",
   "metadata": {},
   "source": [
    "## Parte 2: Generación de Embeddings\n",
    "### Actividad\n",
    "\n",
    "1. Usa dos modelos de sentence-transformers. Puedes usar: `'all-MiniLM-L6-v2'` (SBERT), o `'intfloat/e5-base'` (E5). Cuando uses E5, antepon `\"passage: \"` a cada documento antes de codificar.\n",
    "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
    "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa429c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525ae7515c6169d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roble\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\roble\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roble\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|██████████| 63/63 [1:42:23<00:00, 97.51s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de embeddings: (2000, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Elegimos el modelo: SBERT o E5\n",
    "model_name = 'all-MiniLM-L6-v2'      # SBERT\n",
    "# model_name = 'intfloat/e5-base'    # E5 (recuerda añadir \"passage: \" a cada texto)\n",
    "\n",
    "# Cargamos el modelo seleccionado\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# 2. Preparamos los textos: si es E5, anteponemos \"passage: \" a cada documento\n",
    "if 'e5' in model_name:\n",
    "    inputs = [f'passage: {doc}' for doc in corpus]\n",
    "else:\n",
    "    inputs = corpus\n",
    "\n",
    "# 3. Generamos los embeddings para todos los documentos\n",
    "#    Ajusta batch_size si tu GPU/CPU lo requiere\n",
    "embeddings_list = model.encode(\n",
    "    inputs,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# 4. Convertimos la lista de embeddings a un array de NumPy\n",
    "embeddings = np.array(embeddings_list)\n",
    "\n",
    "# 5. (Opcional) Guardar el array en disco para uso posterior\n",
    "np.save('embeddings.npy', embeddings)\n",
    "\n",
    "# Verificamos la forma final: debería ser (2000, dimensión_del_modelo)\n",
    "print('Shape de embeddings:', embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b50365064d2b1",
   "metadata": {},
   "source": [
    "## Parte 3: Indexación con FAISS\n",
    "### Actividad\n",
    "\n",
    "1. Crea un índice plano con faiss.IndexFlatL2 para búsquedas por distancia euclidiana.\n",
    "2. Asegúrate de usar la dimensión correcta `(embedding_dim = doc_embeddings.shape[1])`.\n",
    "3. Agrega los vectores de documentos al índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c723e6189ab1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40462a067ca2d379",
   "metadata": {},
   "source": [
    "## Parte 4: Consulta Semántica\n",
    "### Actividad\n",
    "\n",
    "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
    "\n",
    "    * \"God, religion, and spirituality\"\n",
    "    * \"space exploration\"\n",
    "    * \"car maintenance\"\n",
    "\n",
    "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon `\"query: \"` a la consulta.\n",
    "3. Recupera los 5 documentos más relevantes con `index.search(...)`.\n",
    "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad085806124c709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc9e5e7815c7508",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
